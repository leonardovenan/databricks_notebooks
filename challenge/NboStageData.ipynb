{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94d96f0-925f-4a99-a730-f5bc42f908e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "%pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3077ac43-af16-4e73-8cac-5f2f079cf6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_path = \"test.xlsx\"\n",
    "df_pd = pd.read_excel(excel_path) \n",
    "\n",
    "df_pd_cleaned = df_pd.astype(str).fillna(\"\") # convert all columns to string\n",
    "\n",
    "df = spark.createDataFrame(df_pd_cleaned)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de53d9a7-202f-44c3-836c-bd052cff2e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need to have some way of handling column names. I prefer to do it automatically.\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def camel_to_snake(name: str) -> str:\n",
    "    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower()\n",
    "\n",
    "def normalize_spark_columns_names(df):\n",
    "    new_column_names = []\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        # camelCase -> snake_case\n",
    "        col_name = camel_to_snake(col_name)\n",
    "        # Espaços para underline\n",
    "        col_name = col_name.replace(' ', '_')\n",
    "        # Somente letras/números/_ com regex\n",
    "        col_name = re.sub(r'\\W+', '_', col_name)\n",
    "        # Remover acentuação\n",
    "        col_name = unidecode(col_name)\n",
    "        # Tudo minúsculo (redundante, mas seguro)\n",
    "        col_name = col_name.lower()\n",
    "        new_column_names.append(col_name)\n",
    "\n",
    "    # Renomear colunas no DataFrame\n",
    "    for old_name, new_name in zip(df.columns, new_column_names):\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad1844b-3af3-4a61-8d19-86f9ad82a947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = normalize_spark_columns_names(df)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57add4e9-0f7e-4024-bfa4-4f0b6219bad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48bf4fbd-349e-4ce4-b310-bc1dec85e095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05d8e98-7a2b-4802-af3a-f36b3f72a18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"staging.test_customer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5cb3fa-a2cb-4e4c-8d80-1b9047afc0c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from staging.test_customer_data"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5906616907733707,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "NboStageData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
